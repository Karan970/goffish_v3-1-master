package in.dream_lab.hadoopPipeline.cc;




import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class PartitionInputReader {

public static void main(String[] args) throws Exception{
		
		int DEFAULT_NO_OF_REDUCERS=24;
		Configuration conf = new Configuration();
		//JOB1 Make Graph Undirected(optional) and Generate EdgeID's
		FileSystem fs = FileSystem.get(conf);
		 //Set parameter whether to make graph undirected 
		conf.set("makeUndirected", args[1]);
		conf.set("GraphID", args[0]);
		conf.set("partition_count", args[4]);
		 // get the number of partitions 
		int numberofPartitions=Integer.parseInt(args[4]) ;
		DEFAULT_NO_OF_REDUCERS = numberofPartitions;
		 
		String job1OutDir=args[0]+"/Job1/";
		 //String job1OutDir="SNAP_DATA/Friendster-Undirected/";
		Path OUTPUT_PATH1=new Path(job1OutDir);
	    Job job1 = Job.getInstance(conf, "conf1");
	    //Set the number of reducer according to the graph size
	    job1.setNumReduceTasks(DEFAULT_NO_OF_REDUCERS);
	    job1.setJarByClass(MakeGraphUndirectedMapper.class);
	    job1.setMapperClass(MakeGraphUndirectedMapper.class);

	    job1.setReducerClass(MakeGraphUndirectedReducer.class);

	    job1.setOutputKeyClass(LongWritable.class);
	    job1.setOutputValueClass(LongWritable.class);

		job1.setMapOutputValueClass(LongWritable.class);

	    FileInputFormat.addInputPath(job1, new Path(args[2]));
	    FileOutputFormat.setOutputPath(job1, OUTPUT_PATH1);

	    job1.waitForCompletion(true);
//	    //System.exit(job1.waitForCompletion(true) ? 0 : 1);
//
//		/*JOB2 :Include vertex partition info into adjacency list*/
		Job job2 = Job.getInstance(conf, "conf2");
		//Set the number of reducer according to the graph size
	    job2.setNumReduceTasks(DEFAULT_NO_OF_REDUCERS);
		job2.setJarByClass(PartitionFileMapper.class);
		//First Argument is file generated by Partitioner(Spinner,METIS,etc)
		MultipleInputs.addInputPath(job2,new Path(args[3]),TextInputFormat.class,PartitionFileMapper.class);
		//Second Argument is file generated by first Job
		MultipleInputs.addInputPath(job2,OUTPUT_PATH1,TextInputFormat.class,ALFileMapper.class);

		job2.setMapOutputKeyClass(LongWritable.class);
		job2.setMapOutputValueClass(Text.class);

		 String job2OutDir=args[0]+"/Job2/";
		 Path OUTPUT_PATH2=new Path(job2OutDir);

		 FileOutputFormat.setOutputPath(job2, OUTPUT_PATH2);
		 job2.setReducerClass(PALReducer.class);
		 job2.setOutputKeyClass(Text.class);
		 job2.setOutputValueClass(Text.class);
		 job2.waitForCompletion(true);

	fs.delete(OUTPUT_PATH1, true);

//
//		//JOB3 COnnected Components within a partition
		Job job3 = Job.getInstance(conf, "conf3");
	    //Here the number of reducers should be set equal to the number of partitions required
		//int numberofPartitions=Integer.parseInt(args[4]) ;
		job3.setNumReduceTasks(numberofPartitions);
	    job3.setJarByClass(CCMapper.class);
	    job3.setMapperClass(CCMapper.class);

	    job3.setReducerClass(CCReducer.class);
	    job3.setOutputKeyClass(Text.class);
	    job3.setOutputValueClass(Text.class);

		job3.setMapOutputValueClass(Text.class);
		job3.setMapOutputKeyClass(LongWritable.class);

	    FileInputFormat.addInputPath(job3, OUTPUT_PATH2);

	    String job3OutDir=args[0]+"/Job3/";
	    Path OUTPUT_PATH3=new Path(job3OutDir);
	    FileOutputFormat.setOutputPath(job3, OUTPUT_PATH3);
	   //These Paths are defined in CCReducer Make sure to change it everyNew Run.
	    //TODO : Take this as command line argument
	   // MultipleOutputs.addNamedOutput(job3, "SPVL", TextOutputFormat.class, Text.class, Text.class);
	    //MultipleOutputs.addNamedOutput(job3, "SPEL", TextOutputFormat.class, Text.class, Text.class);
	    MultipleOutputs.addNamedOutput(job3, "SPLAL", TextOutputFormat.class, Text.class, Text.class);
	    MultipleOutputs.addNamedOutput(job3, "SPRAL", TextOutputFormat.class, Text.class, Text.class);
	    job3.waitForCompletion(true) ;
	    
	  
//	    fs.delete(OUTPUT_PATH1, true);
//	  fs.delete(OUTPUT_PATH2, true);
//	  fs.delete(OUTPUT_PATH3, true);
		
	  //System.exit(job3.waitForCompletion(true) ? 0 : 1);
//	JOB4	
	    String job4OutDir=args[0]+"/Job4/";
	    Path OUTPUT_PATH4=new Path(job4OutDir);
		Job job4 = Job.getInstance(conf, "conf4");
	    job4.setNumReduceTasks(DEFAULT_NO_OF_REDUCERS);
	    job4.setJarByClass(SPRALMapper.class);
	    job4.setMapperClass(SPRALMapper.class);

	    job4.setReducerClass(SPRSALReducer.class);
	    job4.setOutputKeyClass(Text.class);
	    job4.setOutputValueClass(Text.class);

	    job4.setMapOutputKeyClass(LongWritable.class);
		job4.setMapOutputValueClass(Text.class);
//	    //The input path used here is defined in previous job's reducer code
		String INPUT_PATH_FOR_SPRAL=args[0]+"/Job3/"+args[0]+"/SPRAL";

	    FileInputFormat.addInputPath(job4, new Path(INPUT_PATH_FOR_SPRAL));
	    FileOutputFormat.setOutputPath(job4, OUTPUT_PATH4);

	    job4.waitForCompletion(true);
//	    System.exit(job4.waitForCompletion(true) ? 0 : 1);

//	//JOB 5

		Job job5 = Job.getInstance(conf, "conf5");
		job5.setNumReduceTasks(DEFAULT_NO_OF_REDUCERS);
		job5.setJarByClass(SPRSALFileMapper.class);
		MultipleInputs.addInputPath(job5,OUTPUT_PATH4,TextInputFormat.class,SPRSALFileMapper.class);

		String INPUT_PATH_FOR_SPLAL=args[0]+"/Job3/"+args[0]+"/SPLAL";
		MultipleInputs.addInputPath(job5,new Path(INPUT_PATH_FOR_SPLAL),TextInputFormat.class,SPLALFileMapper.class);

		job5.setMapOutputKeyClass(LongWritable.class);
		job5.setMapOutputValueClass(Text.class);

		  String job5OutDir=args[0]+"/Job5/";
		Path OUTPUT_PATH5=new Path(job5OutDir);
//
		FileOutputFormat.setOutputPath(job5, OUTPUT_PATH5);
		 job5.setReducerClass(SPRLALReducer.class);
		 job5.setOutputKeyClass(Text.class);
		 job5.setOutputValueClass(Text.class);
		 job5.waitForCompletion(true);
//	 System.exit(job5.waitForCompletion(true) ? 0 : 1);
	 
	fs.delete(OUTPUT_PATH4, true);
	fs.delete(OUTPUT_PATH3, true);
	fs.delete(OUTPUT_PATH2,true);
	
	
	//Sort Job
	Job job6 = Job.getInstance(conf, "conf6");
        job6.setNumReduceTasks(DEFAULT_NO_OF_REDUCERS);
        job6.setJarByClass(SortReducer.class);
        
        job6.setMapperClass(SortMapper.class);
        job6.setReducerClass(SortReducer.class);
        job6.setOutputKeyClass(Text.class);
        job6.setOutputValueClass(Text.class);

        job6.setMapOutputKeyClass(LongWritable.class);
        job6.setMapOutputValueClass(Text.class);

        FileInputFormat.addInputPath(job6, OUTPUT_PATH5);
        String job6OutDir = args[0]+"/Job6/";
        Path OUTPUT_PATH6 = new Path (job6OutDir);
        FileOutputFormat.setOutputPath(job6, OUTPUT_PATH6);

        job6.waitForCompletion(true);
	 //fs.delete(arg0, arg1);
////Slice Generation	
//	Job job6 = Job.getInstance(conf, "conf6");
//    job6.setNumReduceTasks(numberofPartitions);
//    job6.setJarByClass(GeneratePartitionSliceReducer.class);
//    job6.setMapperClass(GeneratePartitionSliceMapper.class);
//  
//    job6.setReducerClass(GeneratePartitionSliceReducer.class);
//    job6.setOutputKeyClass(Text.class);
//    job6.setOutputValueClass(Text.class);
//    
//    job6.setMapOutputKeyClass(IntWritable.class);  
//	job6.setMapOutputValueClass(Text.class);
//    
//    FileInputFormat.addInputPath(job6, OUTPUT_PATH5);
//    
//	  String job6OutDir=args[0]+"/Job6/";
//	Path OUTPUT_PATH6=new Path(job6OutDir);
//    
//    FileOutputFormat.setOutputPath(job6, OUTPUT_PATH6);
//   
//    System.exit(job6.waitForCompletion(true) ? 0 : 1);
		
		
	}
	
	
}
